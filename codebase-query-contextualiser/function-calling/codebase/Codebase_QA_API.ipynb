{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (0.115.7)\n",
      "Requirement already satisfied: uvicorn in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (0.34.0)\n",
      "Requirement already satisfied: langchain in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-groq in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-openai in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (0.3.4)\n",
      "Requirement already satisfied: faiss-cpu in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from fastapi) (0.45.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from fastapi) (2.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: click>=7.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from uvicorn) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (0.3.34)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (0.2.11)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: requests<3,>=2 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (9.0.0)\n",
      "Requirement already satisfied: numpy<3,>=1.26.2 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain) (2.2.1)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain-groq) (0.15.0)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.58.1 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain-openai) (1.61.1)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain-openai) (0.8.0)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
      "Requirement already satisfied: tqdm>4 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (3.0.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\anaconda\\envs\\gen_ai\\lib\\site-packages (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn langchain langchain-groq langchain-openai faiss-cpu\n",
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API Key Loaded Successfully (Hidden)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API Key from .env file\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Verify if the key is loaded correctly\n",
    "if groq_api_key:\n",
    "    print(\"✅ API Key Loaded Successfully (Hidden)\")\n",
    "else:\n",
    "    print(\"⚠️ Error: API Key Not Found!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Quantum computing! It\\'s a fascinating topic that can seem complex, but I\\'ll try to break it down in simple terms.\\n\\n**What is Quantum Computing?**\\n\\nQuantum computing is a new way of processing information using the principles of quantum mechanics. In traditional computing, information is processed using bits, which can have a value of either 0 or 1. However, in quantum computing, information is processed using quantum bits or qubits, which can exist in multiple states at the same time, known as superposition.\\n\\n**How does it work?**\\n\\nImagine you have a coin that can either be heads or tails. In traditional computing, the coin can only be one or the other. But in quantum computing, the coin can be both heads and tails at the same time! This is because qubits can exist in multiple states simultaneously, which allows for much faster processing of certain types of information.\\n\\n**Key Principles:**\\n\\n1. **Superposition**: Qubits can exist in multiple states at the same time, allowing for faster processing of complex calculations.\\n2. **Entanglement**: Qubits can be connected in such a way that the state of one qubit is dependent on the state of the other, even if they\\'re separated by large distances.\\n3. **Quantum Measurement**: When a qubit is measured, its state \"collapses\" to one of the possible outcomes, similar to how the coin\\'s state is determined when it\\'s flipped and landed.\\n\\n**What are the benefits?**\\n\\n1. **Faster Processing**: Quantum computers can process certain types of information much faster than classical computers, making them useful for complex calculations, such as:\\n\\t* Simulating complex systems (e.g., molecules, weather patterns)\\n\\t* Breaking complex codes (e.g., encryption)\\n\\t* Optimizing complex systems (e.g., logistics, finance)\\n2. **Increased Efficiency**: Quantum computers can process multiple calculations simultaneously, reducing the need for repetitive processing.\\n\\n**Challenges:**\\n\\n1. **Error Correction**: Quantum computers are prone to errors due to the fragile nature of qubits. Developing reliable error correction methods is essential.\\n2. **Scalability**: Currently, quantum computers are small-scale and need to be scaled up to process complex tasks.\\n3. **Quantum Noise**: Quantum computers are sensitive to their environment, which can cause errors. Developing methods to mitigate this noise is crucial.\\n\\n**In Simple Terms:**\\n\\nQuantum computing is a new way of processing information using qubits, which can exist in multiple states at the same time. This allows for faster processing of certain types of information, making it useful for complex calculations. However, developing reliable error correction methods, scaling up quantum computers, and mitigating quantum noise are significant challenges to overcome.\\n\\nI hope this explanation was helpful in understanding quantum computing in simple terms!' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 571, 'prompt_tokens': 18, 'total_tokens': 589, 'completion_time': 0.475833333, 'prompt_time': 0.002487034, 'queue_time': 0.020358291, 'total_time': 0.478320367}, 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-37fad530-f5ea-4d63-a724-39c021845b46-0' usage_metadata={'input_tokens': 18, 'output_tokens': 571, 'total_tokens': 589}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# Test the model\n",
    "response = llm.invoke(\"Explain Quantum Computing in simple terms.\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Directory not found: 'C:/Users/YourUsername/HackathonProject'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m project_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/YourUsername/HackathonProject\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your actual project path\u001b[39;00m\n\u001b[0;32m     19\u001b[0m loader \u001b[38;5;241m=\u001b[39m DirectoryLoader(project_directory, glob\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**/*.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m documents \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 🔹 Step 2: Split the code into smaller chunks\u001b[39;00m\n\u001b[0;32m     23\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Gen_AI\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:117\u001b[0m, in \u001b[0;36mDirectoryLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load documents.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32md:\\anaconda\\envs\\Gen_AI\\Lib\\site-packages\\langchain_community\\document_loaders\\directory.py:123\u001b[0m, in \u001b[0;36mDirectoryLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath)\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDirectory not found: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected directory, got file: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Directory not found: 'C:/Users/YourUsername/HackathonProject'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 🔹 Load API Key from .env file\n",
    "load_dotenv()\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# 🔹 Initialize AI Model (Llama3-8b-8192)\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")\n",
    "\n",
    "# 🔹 Step 1: Load the codebase from your project directory\n",
    "project_directory = \"C:/Users/YourUsername/HackathonProject\"  # Change this to your actual project path\n",
    "loader = DirectoryLoader(project_directory, glob=\"**/*.py\")\n",
    "documents = loader.load()\n",
    "\n",
    "# 🔹 Step 2: Split the code into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "code_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "# 🔹 Step 3: Convert code into embeddings & store in FAISS vector database\n",
    "vector_db = FAISS.from_documents(code_chunks, OpenAIEmbeddings())\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "# 🔹 Step 4: Create a Q&A System for the Codebase\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)\n",
    "\n",
    "# 🔹 Function to ask questions about the codebase\n",
    "def ask_codebase(query):\n",
    "    \"\"\"Ask AI a question about the codebase and get a response.\"\"\"\n",
    "    response = qa_chain.run(query)\n",
    "    return response\n",
    "\n",
    "# 🔹 Example: Asking questions about the codebase\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"✅ Codebase Q&A Assistant is Ready!\")\n",
    "    while True:\n",
    "        query = input(\"\\nAsk about the codebase (or type 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        answer = ask_codebase(query)\n",
    "        print(f\"\\n🔹 AI Answer: {answer}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gen_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
